{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 29047,
          "sourceType": "datasetVersion",
          "datasetId": 22655
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Semantic Segmentation: U-Net",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanmaharaj/CVIT-AI-Summer-School/blob/main/Semantic_Segmentation_U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'cityscapes-image-pairs:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F22655%2F29047%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240717%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240717T123613Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6cf8f1b68abf12b2239a30e6cde9ab9bd79ce7c442613147b75306caf8ffe4a59787e596e8090848bab1226d95498b77b9d811fc37a9602a61c2f93006974f1a82de39dd53468bf145035b861d8ee8a01f9adfff2c2a58f1eb16d56f2147bdb476ef1e5df6a4f5058e7e26c595d1a1bb2e8e2ba7fd80a01812f007f85e030c710bf2074a88408e32d319f67740ae766deed9f6bf17275756532aa024fba9cbb119db1cf5b180bbfdea5a973ddd4d787768727e0144aa410bece042920e2cbf2a220575d107703245b45b36439cb8e6a65ca6fdeffa2fbe3c6edb5f3974e9705d0f88dc0c5ddf6c24e3fc16f74eaadf55d04eb8a3c815bef5714fa8be3810b425'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "fu9IkpkJsYUs"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Segmentation\n",
        "\n",
        "Semantic segmentation is a computer vision task that involves assigning a label to each pixel in an image, with the goal of segmenting the image into meaningful regions. It is a fundamental problem in understanding and analyzing images, with applications in various fields such as autonomous driving, medical imaging, and scene understanding.\n",
        "\n",
        "The input to the network had to be rescaled because of the memory and GPU hours limitations from (256, 256,3) to (128,128, 3).\n",
        "\n",
        "![intro.jpg](attachment:4cb48f7f-8fc5-490c-94ad-f4bf447e5f69.jpg)\n",
        "\n",
        "In this tutorial, we'll cover the following steps to build a semantic segmentation model using PyTorch:\n",
        "1. Dataset Preparation and visualization\n",
        "2. Discussion on evaluation metrics\n",
        "3. Writing evaluation and training methods\n",
        "4. Model definition and training\n",
        "5. Inference on New Images\n",
        "6. Effect of different loss functions"
      ],
      "metadata": {
        "id": "xOi_vZNmsYUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "We start by installing extra dependencies and importing relevant libraries"
      ],
      "metadata": {
        "id": "SoIVePrKsYUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install extra dependencies\n",
        "!pip install -q torchinfo accelerate tqdm\n",
        "\n",
        "import os\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np # linear algebra\n",
        "\n",
        "# pytorch dependency\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "\n",
        "# HuggingFace accelerate library\n",
        "from accelerate import Accelerator # (easy support for multiple GPU's, TPU, floating point 16s, which makes training much faster)\n",
        "\n",
        "# displaying the pytorch architecture (makes prototyping the network easier, as it shows shapes)\n",
        "from torchinfo import summary\n",
        "\n",
        "# plotting the results\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# creatation and transformations for the dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# show nice progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from time import time"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:31:03.812563Z",
          "iopub.execute_input": "2024-07-04T07:31:03.812972Z",
          "iopub.status.idle": "2024-07-04T07:31:24.644101Z",
          "shell.execute_reply.started": "2024-07-04T07:31:03.812913Z",
          "shell.execute_reply": "2024-07-04T07:31:24.64274Z"
        },
        "trusted": true,
        "id": "UijBKH7gsYUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config\n",
        "Let us now define some hyperparameters and global variables in a configuration file"
      ],
      "metadata": {
        "id": "4akILwXIsYUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CONFIG:\n",
        "    # use the 16 bit floating point arithmetic (should speeds up training/inference)\n",
        "    USE_MIXED_PRECISION = \"fp16\"    # other values possible, \"fp16\" or None\n",
        "\n",
        "    # downscaling the images : to make the inference on kaggle faster and keep within reason on GPU there,\n",
        "    #                          I set it to 2 (so the image is scaled from (256,256) to (128, 128)), None\n",
        "    #                          keeps the original shape\n",
        "    DOWNSCALE = 2\n",
        "\n",
        "    # Imagenet channelwise mean\n",
        "    MEAN = [0.485, 0.456, 0.406]\n",
        "\n",
        "    # imagenet, channelwise standard deviation\n",
        "    STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # epsilon for DICE, IoU losses (now 1e-6, however in some papers set to 1)\n",
        "    EXTRA_LOSS_EPS = 1e-6\n",
        "\n",
        "    # style of plots, I find darkgrid nice for regular plots\n",
        "    SNS_STYLE = \"darkgrid\"\n",
        "\n",
        "    BATCH_SIZE = 8\n",
        "\n",
        "    SINGLE_NETWORK_TRAINING_EPOCHS = 15\n",
        "\n",
        "    CE_VS_DICE_EVAL_EPOCHS = 15\n",
        "\n",
        "    DELTA_BETA = 0.2\n",
        "\n",
        "cfg = CONFIG()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:31:24.645988Z",
          "iopub.execute_input": "2024-07-04T07:31:24.646473Z",
          "iopub.status.idle": "2024-07-04T07:31:24.653107Z",
          "shell.execute_reply.started": "2024-07-04T07:31:24.646445Z",
          "shell.execute_reply": "2024-07-04T07:31:24.652Z"
        },
        "trusted": true,
        "id": "kq7lUflGsYUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset - CityScapes Image Pairs\n",
        "\n",
        "### Context\n",
        "Cityscapes data [(dataset home page)](https://www.cityscapes-dataset.com/) contains labeled videos taken from vehicles driven in Germany. This version is a processed subsample created as part of the [Pix2Pix paper](https://phillipi.github.io/pix2pix/). The dataset has still images from the original videos, and the semantic segmentation labels are shown in images alongside the original image.\n",
        "\n",
        "### Content\n",
        "This dataset has 2975 training images files and 500 validation image files. Each image file is 256x512 pixels, and each file is a composite with the original photo on the left half of the image, alongside the labeled image (output of semantic segmentation) on the right half."
      ],
      "metadata": {
        "id": "k4rSMIR0sYU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = os.path.join(\"/kaggle\", \"input\", \"cityscapes-image-pairs\", \"cityscapes_data\")\n",
        "\n",
        "# setting up the datapaths\n",
        "train_datapath = os.path.join(datapath, \"train\")\n",
        "val_datapath = os.path.join(datapath, \"val\")\n",
        "train_cs_datapath = os.path.join(datapath, \"cityscapes_data\", \"train\")\n",
        "val_cs_datapath = os.path.join(datapath, \"cityscapes_data\", \"val\")\n",
        "\n",
        "# list all, full datapaths for training and validation images and save them in these two variables\n",
        "training_images_paths = [os.path.join(train_datapath, f) for f in os.listdir(train_datapath)]\n",
        "validation_images_paths = [os.path.join(val_datapath, f) for f in os.listdir(val_datapath)]\n",
        "\n",
        "# sanity check, how many images\n",
        "print(f\"size of training : {len(training_images_paths)}\")\n",
        "print(f\"size of cityscapes training : {len(os.listdir(train_cs_datapath))}\")\n",
        "print(f\"size of validation : {len(validation_images_paths)}\")\n",
        "print(f\"size of cityscapes validation : {len(os.listdir(val_cs_datapath))}\")\n",
        "\n",
        "global_step = 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:31:24.654361Z",
          "iopub.execute_input": "2024-07-04T07:31:24.654631Z",
          "iopub.status.idle": "2024-07-04T07:31:26.0295Z",
          "shell.execute_reply.started": "2024-07-04T07:31:24.654608Z",
          "shell.execute_reply": "2024-07-04T07:31:26.028527Z"
        },
        "trusted": true,
        "id": "-MBF0nszsYU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a HuggingFace Accelerate accelerator. This allows using multiple GPUs, TPUs or\n",
        "# mixed precision (like brain-float16 or 16bit floating points) which should make training/inference\n",
        "# faster\n",
        "if cfg.USE_MIXED_PRECISION is not None:\n",
        "    accelerator = Accelerator(mixed_precision=cfg.USE_MIXED_PRECISION)\n",
        "else:\n",
        "    accelerator = Accelerator()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:31:26.03173Z",
          "iopub.execute_input": "2024-07-04T07:31:26.032041Z",
          "iopub.status.idle": "2024-07-04T07:31:26.093089Z",
          "shell.execute_reply.started": "2024-07-04T07:31:26.031997Z",
          "shell.execute_reply": "2024-07-04T07:31:26.092317Z"
        },
        "trusted": true,
        "id": "X0JBxpuLsYU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data visualization\n",
        "Firstly, inspect the dataset to see what the data looks like. Below few random images from the dataset are plotted <br>\n",
        "Unfortunately in this dataset, we are not provided with separate mask data, but data is a single image which is a concatenation of input image and RGB mask representation. This makes it trickier to work with"
      ],
      "metadata": {
        "id": "RElcTj-XsYU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many images (total width * height)\n",
        "width = 4\n",
        "height = 4\n",
        "vis_batch_size = width * height\n",
        "\n",
        "# get vis_batch_size unique, random indices\n",
        "indexes = np.arange(len(training_images_paths))\n",
        "indexes = np.random.permutation(indexes)[:vis_batch_size]\n",
        "\n",
        "# create the plot\n",
        "fig, axs = plt.subplots(height, width, sharex=True, sharey=True, figsize=(16, 8))\n",
        "for i in range(vis_batch_size):\n",
        "    # read the image\n",
        "    img = torchvision.io.read_image(training_images_paths[indexes[i]])\n",
        "\n",
        "    # pytorch reads it as (c, h, w), reshape it to (h, w, c) which is the shape matplotlib wants\n",
        "    img = img.permute(1, 2, 0)\n",
        "\n",
        "    # calculate the indexes for plots and set the image data\n",
        "    y, x = i // width, i % width\n",
        "    axs[y, x].imshow(img.numpy())\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:31:26.094159Z",
          "iopub.execute_input": "2024-07-04T07:31:26.094689Z",
          "iopub.status.idle": "2024-07-04T07:31:30.53148Z",
          "shell.execute_reply.started": "2024-07-04T07:31:26.09465Z",
          "shell.execute_reply": "2024-07-04T07:31:30.530304Z"
        },
        "trusted": true,
        "id": "2z5GuojRsYU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing\n",
        "#### labels\n",
        "To restore the categories information from a single jpg image, we firstly get the names and categories from the cited cityscapes repository. We get the color representation of each of these classes, and save it to *idx_to_color*. For each pixel then of an input jpg image, we find a closest color, and put there the index of category which represent this color. this will give us the required (height, width, num_of_classes) representation.\n",
        "\n",
        "This is achieved by code below"
      ],
      "metadata": {
        "id": "ssb7MD5YsYU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference\n",
        "# link : https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py\n",
        "idx_to_name = [ 'unlabeled','ego vehicle','rectification border', 'out of roi', 'static', 'dynamic','ground', 'road', 'sidewalk', 'parking', 'rail track', 'building', 'wall', 'fence','guard rail' , 'bridge','tunnel','pole', 'polegroup', 'traffic light', 'traffic sign' ,'vegetation', 'terrain', 'sky' ,'person', 'rider', 'car','truck' ,'bus', 'caravan','trailer', 'train' , 'motorcycle','bicycle','license plate']\n",
        "idx_to_category = [\"void\", \"flat\", \"construction\", \"object\", \"nature\", \"sky\", \"human\", \"vehicle\"]\n",
        "\n",
        "idx_to_color = [[ 0,  0,  0], [ 0,  0,  0], [  0,  0,  0], [  0,  0,  0],[ 0,  0,  0],[111, 74,  0],[81,  0, 81] ,[128, 64,128],[244, 35,232],\n",
        "                [250,170,160],[230,150,140],[70, 70, 70],[102,102,156],[190,153,153],[180,165,180],[150,100,100],[150,120, 90],[153,153,153],\n",
        "                [153,153,153],[250,170, 30],[220,220,  0],[107,142, 35],[152,251,152],[ 70,130,180],[220, 20, 60],[255,  0,  0],[ 0,  0,142],\n",
        "                [ 0,  0, 70],[ 0, 60,100],[ 0,  0, 90],[  0,  0,110],[ 0, 80,100],[  0,  0,230],[119, 11, 32],[  0,  0,142]]\n",
        "\n",
        "\n",
        "idx_to_color_np = np.array(idx_to_color)\n",
        "\n",
        "name_to_category = {0 : 0, 1 : 0, 2 : 0, 3: 0, 4 : 0, 5 : 0, 6 : 0, 7 : 1, 8 : 1, 9 : 1, 10 : 1, 11 :2, 12 : 2, 13 : 2, 14 : 2, 15 : 2, 16 : 2,\n",
        "                    17 : 3, 18 : 3, 19 : 3, 20: 3, 21 : 4, 22 : 4, 23 : 5, 24 : 6, 25 : 6, 26 : 7, 27 : 7, 28 : 7, 29 : 7, 30 : 7, 31 : 7, 32: 7, 33 : 7, 34 : 7}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:31:30.532716Z",
          "iopub.execute_input": "2024-07-04T07:31:30.533007Z",
          "iopub.status.idle": "2024-07-04T07:31:30.547074Z",
          "shell.execute_reply.started": "2024-07-04T07:31:30.532982Z",
          "shell.execute_reply": "2024-07-04T07:31:30.546176Z"
        },
        "trusted": true,
        "id": "tr4ipbWNsYU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "# vectorize the operation of getting the name to category for numpy (just a lookup in name_to_category dictionary)\n",
        "name_to_category_mapping = lambda x: name_to_category[x]\n",
        "vectorized_cat_mapping = np.vectorize(name_to_category_mapping)\n",
        "\n",
        "# vectorize the operation of mapping the name to color for numpy (just a lookup in idx_to_color dictionary)\n",
        "name_to_col_mapping = lambda x: idx_to_color[x]\n",
        "vectorized_col_mapping = np.vectorize(name_to_col_mapping)\n",
        "\n",
        "def preprocess_image(path : str, sparse_mapping=True, downscale_factor=None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "        Read the .jpeg image from *path*. Return the input image (256 x 256 x 3), mask (256 x 256 x 3) read from the jpeg\n",
        "        and conversion to categories or names (if sparse_mapping is true) representation (256 x 256 x (|categories| or |names|) )\n",
        "    \"\"\"\n",
        "    # Read the image from path and dowscale if downscale_factor is not None.\n",
        "    img = Image.open(path)\n",
        "    width, height = img.size\n",
        "\n",
        "    if downscale_factor:\n",
        "        width, height = width // downscale_factor, height//downscale_factor\n",
        "        img = img.resize(( width, height ))\n",
        "\n",
        "    # then split the image into two images (in the middle of width) : input image and color mask (each represented by 3 channels)\n",
        "    img = np.asarray(img)\n",
        "    raw, mask = img[:, :width//2, :], img[:, width//2:, :]\n",
        "\n",
        "    height, width, channels = mask.shape\n",
        "\n",
        "    # compute then the sum of squared distances for each pixel to the colors (L2 between the color and pixel data) :\n",
        "    # the value which will be the minimal is the category name we will use for that pixel, and we will get it using argmin\n",
        "    distances = np.sum((mask.reshape(-1, channels)[:, np.newaxis, :] - idx_to_color_np)**2, axis=2)\n",
        "    classes = np.argmin(distances, axis=1).reshape(height, width)\n",
        "\n",
        "    # if we want to operate on names, map the categories to names\n",
        "    if sparse_mapping:\n",
        "        classes = vectorized_cat_mapping(classes)\n",
        "\n",
        "    return raw, mask, classes\n",
        "\n",
        "\n",
        "x, mask_raw, classes = preprocess_image(training_images_paths[indexes[i]], sparse_mapping=False, downscale_factor=None)\n",
        "\n",
        "# sanity checks and print the data\n",
        "print(\"size of input : \", x.shape)\n",
        "print(\"size of mask raw : \", mask_raw.shape)\n",
        "print(\"size of classes : \", classes.shape)\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(x)\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(mask_raw)\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(classes)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:31:30.548471Z",
          "iopub.execute_input": "2024-07-04T07:31:30.548765Z",
          "iopub.status.idle": "2024-07-04T07:31:31.113841Z",
          "shell.execute_reply.started": "2024-07-04T07:31:30.548741Z",
          "shell.execute_reply": "2024-07-04T07:31:31.112777Z"
        },
        "trusted": true,
        "id": "xxTnM-OxsYU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data and Pytorch dataset\n",
        "In the section below, we will create and check the Pytorch dataset for this images"
      ],
      "metadata": {
        "id": "y76XrjQGsYU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load dataset\n",
        "There aren't that many images for training/validation and the size of them is pretty small, therefore to speed up the computation, we can\n",
        "just load them into RAM. This will be achieved by preprocessing as above all paths, and appending the input images and proper masks to python arrays <br><br>\n",
        "The images themselves are scaled to [0, 1] and converted to PyTorch Tensors (with the (c, h, w) convention )"
      ],
      "metadata": {
        "id": "d-XBKPSXsYU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_to_use = -1\n",
        "\n",
        "# for kaggle\n",
        "downscale_factor=cfg.DOWNSCALE\n",
        "\n",
        "X_train, Y_train = [], []\n",
        "X_val, Y_val = [], []\n",
        "\n",
        "for path in tqdm(training_images_paths[:]):\n",
        "    X, _, Y = preprocess_image(path, downscale_factor=downscale_factor)\n",
        "    X_train.append(torch.Tensor(X / 255.).permute(2, 0, 1))\n",
        "    Y_train.append(torch.Tensor(Y))\n",
        "\n",
        "for path in tqdm(validation_images_paths):\n",
        "    X, _, Y = preprocess_image(path, downscale_factor=downscale_factor)\n",
        "    X_val.append(torch.Tensor(X / 255.).permute(2, 0, 1))\n",
        "    Y_val.append(torch.Tensor(Y))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:31:31.115112Z",
          "iopub.execute_input": "2024-07-04T07:31:31.115427Z",
          "iopub.status.idle": "2024-07-04T07:33:50.884682Z",
          "shell.execute_reply.started": "2024-07-04T07:31:31.1154Z",
          "shell.execute_reply": "2024-07-04T07:33:50.883704Z"
        },
        "trusted": true,
        "id": "mBrkZNC3sYU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"size of X_train : {len(X_train)} ; Y_train {len(Y_train)}\")\n",
        "print(f\"size of X_val : {len(X_val)} ; Y_val {len(Y_val)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:50.88615Z",
          "iopub.execute_input": "2024-07-04T07:33:50.886644Z",
          "iopub.status.idle": "2024-07-04T07:33:50.892229Z",
          "shell.execute_reply.started": "2024-07-04T07:33:50.886606Z",
          "shell.execute_reply": "2024-07-04T07:33:50.891319Z"
        },
        "trusted": true,
        "id": "3TVC8G6_sYU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_in_B = 0\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    total_size_in_B += X_train[i].element_size() * X_train[i].nelement()\n",
        "    total_size_in_B += Y_train[i].element_size() * Y_train[i].nelement()\n",
        "\n",
        "for i in range(len(X_val)):\n",
        "    total_size_in_B += X_val[i].element_size() * X_val[i].nelement()\n",
        "    total_size_in_B += Y_val[i].element_size() * Y_val[i].nelement()\n",
        "\n",
        "print(f\"The total size of data in RAM is {round(total_size_in_B / 1000 / 1000 / 1000, 3)} GB\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:50.896507Z",
          "iopub.execute_input": "2024-07-04T07:33:50.896847Z",
          "iopub.status.idle": "2024-07-04T07:33:50.910495Z",
          "shell.execute_reply.started": "2024-07-04T07:33:50.896816Z",
          "shell.execute_reply": "2024-07-04T07:33:50.909616Z"
        },
        "trusted": true,
        "id": "bHW-IQI2sYU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pytorch Dataset\n",
        "\n",
        "Create a PyTorch dataset from which we will get the data. It's very simple, just get the image from the python arrays, and apply some preprocessing.<br>\n",
        "Here only the normalization with $\\mu$ and $\\sigma$ calculated from ImageNet is applied. It could be done in loop above, however I used transforms just to keep code somehow more maintanable (and the resulting speed benefit is very small)"
      ],
      "metadata": {
        "id": "gmJN72jKsYU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch Dataset\n",
        "class CityScapesDataset(Dataset):\n",
        "    def __init__(self, X, Y, transform=None, target_transform=None):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.X[idx], self.Y[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        if self.target_transform:\n",
        "            y = self.target_transform(y)\n",
        "        return x , y\n",
        "\n",
        "\n",
        "# just normalize the data\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Normalize(mean=cfg.MEAN, std=cfg.STD),\n",
        "])\n",
        "\n",
        "# create the Datasets\n",
        "train_ds = CityScapesDataset(X_train, Y_train, transform=preprocess)\n",
        "val_ds = CityScapesDataset(X_val, Y_val, transform=preprocess)\n",
        "\n",
        "# create the dataloaders\n",
        "train_dataloader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_ds, batch_size=cfg.BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:50.911403Z",
          "iopub.execute_input": "2024-07-04T07:33:50.911629Z",
          "iopub.status.idle": "2024-07-04T07:33:50.924177Z",
          "shell.execute_reply.started": "2024-07-04T07:33:50.91161Z",
          "shell.execute_reply": "2024-07-04T07:33:50.923118Z"
        },
        "trusted": true,
        "id": "f220lJG7sYU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test dataset\n",
        "Just a sanity check whether the data is loaded correctly. Some artifacts are visible, like in image below. I believe this is fine, given it's only a notebook for fun and playing with some basic dataset (this wouldn't be like this if we have a mask data)"
      ],
      "metadata": {
        "id": "XoVoTNLRsYU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(cfg.BATCH_SIZE, 2, figsize=(4, 2.*cfg.BATCH_SIZE), squeeze=True)\n",
        "fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
        "\n",
        "for i in range(cfg.BATCH_SIZE):\n",
        "    img, mask = X_train[i], Y_train[i]\n",
        "    #print(img.shape, mask.shape)\n",
        "    axes[i, 0].imshow(img.permute(1,2, 0))\n",
        "    axes[i,0].set_xticks([])\n",
        "    axes[i,0].set_yticks([])\n",
        "\n",
        "    axes[i, 1].imshow(mask, cmap='magma')\n",
        "    axes[i,1].set_xticks([])\n",
        "    axes[i,1].set_yticks([])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:50.925206Z",
          "iopub.execute_input": "2024-07-04T07:33:50.925485Z",
          "iopub.status.idle": "2024-07-04T07:33:51.889692Z",
          "shell.execute_reply.started": "2024-07-04T07:33:50.925463Z",
          "shell.execute_reply": "2024-07-04T07:33:51.888762Z"
        },
        "trusted": true,
        "id": "I37B6OWJsYU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save a batch from test dataloader for later evaluation"
      ],
      "metadata": {
        "id": "2I9g0Aj2sYU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_batch_data = next(iter(val_dataloader))\n",
        "\n",
        "# the images coming from the dataset are now preprocessed :\n",
        "# images are normalized using means and standard deviations coming from ImageNet (x' = (x - \\mu) / \\std)\n",
        "# to decode, multiply by standard deviation, and add mean (x = x' * \\std + \\mu)\n",
        "\n",
        "def decode_image(img : torch.Tensor) -> torch.Tensor:\n",
        "    return img * torch.Tensor(cfg.STD) + torch.Tensor(cfg.MEAN)\n",
        "\n",
        "print(eval_batch_data[0].shape, eval_batch_data[1].shape)\n",
        "batch_size = eval_batch_data[0].shape[0]\n",
        "fig, axes = plt.subplots(batch_size, 2, figsize=(4, 2.*batch_size), squeeze=True)\n",
        "fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
        "\n",
        "for i in range(batch_size):\n",
        "    img, mask = eval_batch_data[0][i], eval_batch_data[1][i]\n",
        "    #print(img.shape, mask.shape)\n",
        "    axes[i, 0].imshow(decode_image(img.permute(1,2, 0)))\n",
        "    axes[i,0].set_xticks([])\n",
        "    axes[i,0].set_yticks([])\n",
        "\n",
        "    axes[i, 1].imshow(mask, cmap='magma')\n",
        "    axes[i,1].set_xticks([])\n",
        "    axes[i,1].set_yticks([])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:51.891026Z",
          "iopub.execute_input": "2024-07-04T07:33:51.891367Z",
          "iopub.status.idle": "2024-07-04T07:33:53.190953Z",
          "shell.execute_reply.started": "2024-07-04T07:33:51.891338Z",
          "shell.execute_reply": "2024-07-04T07:33:53.190029Z"
        },
        "trusted": true,
        "id": "girFJRLcsYU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Extra Losses\n",
        "\n",
        "### Brief Description\n",
        "Because our task is pixelwise classification, initially we could think that we could just use crossentropy to predict the class of the pixel. This is used, however there exists also many other losses, which are often either used or combined, to get much better results. <br>\n",
        "Other popular metrics include :\n",
        "- **DICE coefficient (or Sørensen–Dice coefficient)**. This loss is used to compare the pixelwise agreement between the predicted segmentation and mask, and is represented as : $$ DICE(X, X_{truth}) = \\frac{2|X \\cap X_{truth}|}{|X| + |X_{truth}|}$$ This metrics takes values from 0 (no overlap at all) and 1 (full overlap). This loss his widely adapted to calculate the simillarity of two images [2]. DICE loss has a large benefit in comparison to cross entropy, that it considers the loss information both locally and globally (in comparison to crossentropy which cares only about local). It is consider much better loss for semantic segmenation, especially in inbalanced classes problems. <br><br>\n",
        "- **Intersection over Union (or Jaccard coefficient)**. Simillarly to DICE, This loss is also used to compare the pixelwise agreement between the predicted segmenation and mask. It's quite simillar to DICE in it's formulation, and it's given by $$ IoU(X, X_{truth}) = \\frac{|X \\cap X_{truth}|}{|X \\cup X_{truth}|} = \\frac{|X \\cap X_{truth}|}{|X|+ |X_{truth}| - |X \\cap X_{truth}|} $$ This metric is simple, intuitive (devide the Area of Overlap over Area of Union : this achieves a score from 0 (no overlap) to 1 (we perfectly classified everything, and intersection of truth/prediction and union of them are the same)). It provides the same benefits as DICE : it is concerned with both local and global alignment, providing a better value to optimize then cross entropy. Both metrics are used for semantic segmentaion and overall are quite simillar however there is at least one important difference : DICE gives more weight to intersection than IoU (because of the factor 2, which I believe means that some small errors are tolerated in DICE). DICE loss is better suited for handling class imbalance due to its formulation, which gives more weight to the overlap.\n",
        "<br>\n",
        "\n",
        "![dice_iou.png](attachment:e5e43123-3ae1-4db4-9d3c-fa3d82667c99.png)\n",
        "\n",
        "\n",
        "There exists many more losses, many of which are mentioned in really nice survey [2].\n",
        "\n",
        "\n",
        "### Multiclass\n",
        "Important consideration of each of this losses is that most often they are only mentioned for binary classes. Most likely however, we will deal with multiclass prediction problem. A way to deal with it, is to one hot encode our predictions, and calculate DICE or IoU for each of the classes. Then calculate the mean of them (or weighted mean) to get the final score.\n",
        "\n",
        "### great articles / references :\n",
        "- [1]https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\n",
        "- [2]https://arxiv.org/pdf/2006.14822.pdf\n",
        "- [3]https://medium.com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b\n",
        "- [4]https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou/276144#276144"
      ],
      "metadata": {
        "id": "inFD8SKdsYU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dice loss\n",
        "# awesome implementation for DICE can be found here\n",
        "# https://github.com/milesial/Pytorch-UNet/blob/master/utils/dice_score.py\n",
        "def dice_coeff(inp : Tensor, tgt : Tensor, eps=cfg.EXTRA_LOSS_EPS):\n",
        "    sum_dim = (-1, -2, -3)\n",
        "\n",
        "    # calculation of intersection\n",
        "    inter = 2 *(inp * tgt).sum(dim=sum_dim)\n",
        "\n",
        "    # calculate the sum of |inp| + |tgt|\n",
        "    sets_sum = inp.sum(dim=sum_dim) + tgt.sum(dim=sum_dim)\n",
        "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
        "\n",
        "    # calcaute the dice\n",
        "    dice = (inter + eps) / (sets_sum + eps)\n",
        "\n",
        "    # average the dice of classwise\n",
        "    return dice.mean()\n",
        "\n",
        "def multiclass_dice_coeff(input: Tensor, target: Tensor, eps: float = cfg.EXTRA_LOSS_EPS):\n",
        "    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), eps)\n",
        "\n",
        "def dice_loss(input: Tensor, target: Tensor):\n",
        "    # Dice loss (objective to minimize) between 0 and 1\n",
        "    return 1 - multiclass_dice_coeff(input, target)\n",
        "\n",
        "def IoU_coeff(inp : Tensor, tgt : Tensor, eps = 1e-6):\n",
        "    sum_dim = (-1, -2, -3)\n",
        "\n",
        "    # Intersection term  |A ^ B|\n",
        "    inter = (inp * tgt).sum(dim=sum_dim)\n",
        "\n",
        "    # sum of |A| + |B|\n",
        "    sets_sum = inp.sum(dim=sum_dim) + tgt.sum(dim=sum_dim)\n",
        "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
        "\n",
        "    # IoU = |A ^ B| / |A \\/ B| = |A ^ B| / (|A| + |B| - |A^B|)\n",
        "    return (inter + eps) / (sets_sum - inter + eps)\n",
        "\n",
        "def IoU_loss(inp : Tensor, tgt : Tensor):\n",
        "    return 1 - IoU_coeff(inp.flatten(0,1), tgt.flatten(0,1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:53.192339Z",
          "iopub.execute_input": "2024-07-04T07:33:53.192778Z",
          "iopub.status.idle": "2024-07-04T07:33:53.207052Z",
          "shell.execute_reply.started": "2024-07-04T07:33:53.192746Z",
          "shell.execute_reply": "2024-07-04T07:33:53.2059Z"
        },
        "trusted": true,
        "id": "HO9ipODJsYU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Evaluation and Training\n",
        "We will now write a generic function to evaluate and train semantic segmentation models. We will later use these functions to train specific semantic segmentation models.\n",
        "\n",
        "### Model Evaluation"
      ],
      "metadata": {
        "id": "soEYouLOsYU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_dataloader, epoch, epochs, criterion,\n",
        "                   with_dice_loss=True, with_IoU_loss=False):\n",
        "    global epoch_to_fig\n",
        "    val_loss = 0\n",
        "    val_dice = 0\n",
        "    val_IoU = 0\n",
        "    with tqdm(val_dataloader, desc=f\"Epoch {epoch}/{epochs} ; val Loss 0\") as pbar:\n",
        "        model.eval()\n",
        "        examples_so_far = 0\n",
        "        for i, batch in enumerate(val_dataloader):\n",
        "            images, true_masks = batch[0], batch[1]\n",
        "\n",
        "            images = images.to(device)\n",
        "            true_masks = true_masks.to(device).long()\n",
        "\n",
        "            # predictions\n",
        "            masks_pred = model(images)\n",
        "\n",
        "            loss = criterion(masks_pred, true_masks)\n",
        "            val_loss += loss.item() * images.shape[0]\n",
        "            examples_so_far += images.shape[0]\n",
        "\n",
        "            dice = dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
        "                             F.one_hot(true_masks.long(), number_of_classes).permute(0, 3, 1, 2).float())\n",
        "\n",
        "            if with_dice_loss:\n",
        "                loss += dice\n",
        "            val_dice += images.shape[0] * (1.-dice.item())\n",
        "\n",
        "            IoU = IoU_loss(F.softmax(masks_pred, dim=1).float(),\n",
        "                           F.one_hot(true_masks.long(), number_of_classes).permute(0, 3, 1, 2).float())\n",
        "\n",
        "            if with_IoU_loss:\n",
        "                loss += IoU\n",
        "            val_IoU += images.shape[0] * (1.-IoU.item())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "            descr = f\"Epoch {epoch}/{epochs} ; val Loss {round(val_loss / examples_so_far ,3)}, val IoU : {round(val_IoU / examples_so_far ,3)}, val Dice : {round(val_dice / examples_so_far ,3)}\"\n",
        "            pbar.set_description(descr)\n",
        "\n",
        "    eval_summary = {}\n",
        "    eval_summary[\"validation_loss\"] = val_loss / examples_so_far\n",
        "    eval_summary[\"validation_DICE_coefficient\"] = val_dice / examples_so_far\n",
        "    eval_summary[\"validation_IoU_coefficient\"] = val_IoU / examples_so_far\n",
        "\n",
        "    return eval_summary"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:53.208366Z",
          "iopub.execute_input": "2024-07-04T07:33:53.208662Z",
          "iopub.status.idle": "2024-07-04T07:33:53.223778Z",
          "shell.execute_reply.started": "2024-07-04T07:33:53.208631Z",
          "shell.execute_reply": "2024-07-04T07:33:53.222959Z"
        },
        "trusted": true,
        "id": "rHGVDXGGsYU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "_XWCovwpsYU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, device, train_dataloader,\n",
        "                val_dataloader, epochs=10, lr=1e-4, update_pb_every_batch = 1,\n",
        "                with_dice_loss=True, with_IoU_loss=False):\n",
        "    global global_step\n",
        "\n",
        "    # setup the optimizer, loss, learning rate scheduler\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
        "\n",
        "    returned_data = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        training_loss = 0\n",
        "        training_dice = 0\n",
        "        training_IoU = 0\n",
        "        with tqdm(train_dataloader, desc=f\"Epoch {epoch}/{epochs} ; training Loss {round(training_loss,3)}\") as pbar:\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            examples_so_far = 0\n",
        "            for i,batch in enumerate(train_dataloader):\n",
        "                optimizer.zero_grad()\n",
        "                images, true_masks = batch[0], batch[1]\n",
        "\n",
        "                images = images.to(device)\n",
        "                true_masks = true_masks.to(device).long()\n",
        "\n",
        "                masks_pred = model(images)\n",
        "\n",
        "                loss = criterion(masks_pred, true_masks)\n",
        "                if with_dice_loss:\n",
        "                    dice = dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
        "                              F.one_hot(true_masks.long(), number_of_classes).permute(0, 3, 1, 2).float())\n",
        "                    loss += dice\n",
        "                    training_dice += images.shape[0] * (1.-dice.item())\n",
        "\n",
        "                if with_IoU_loss:\n",
        "                    IoU = IoU_loss(F.softmax(masks_pred, dim=1).float(),\n",
        "                              F.one_hot(true_masks.long(), number_of_classes).permute(0, 3, 1, 2).float())\n",
        "                    loss += IoU\n",
        "                    training_IoU += images.shape[0] * (1.-IoU.item())\n",
        "\n",
        "                # Backward and optimize\n",
        "                #loss.backward()\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "\n",
        "                global_step += 1\n",
        "                training_loss += images.shape[0] * loss.item()\n",
        "                examples_so_far += images.shape[0]\n",
        "\n",
        "                pbar.update(1)\n",
        "                if i % update_pb_every_batch == 0:\n",
        "                    descr = f\"Epoch {epoch}/{epochs} ; training Loss {round(training_loss / examples_so_far ,3)}\"\n",
        "                    if with_dice_loss:\n",
        "                        descr = f\"Epoch {epoch}/{epochs} ; training Loss {round(training_loss / examples_so_far ,3)}, avg DICE : {round(training_dice / examples_so_far ,3)}\"\n",
        "                    elif with_IoU_loss:\n",
        "                        descr = f\"Epoch {epoch}/{epochs} ; training Loss {round(training_loss / examples_so_far ,3)}, avg IoU : {round(training_IoU / examples_so_far ,3)}\"\n",
        "\n",
        "                    pbar.set_description(descr)\n",
        "\n",
        "        epoch_summary = {}\n",
        "        epoch_summary[\"training_loss\"] = training_loss / examples_so_far\n",
        "        if with_dice_loss:\n",
        "            epoch_summary[\"training_DICE_coefficient\"] = training_dice / examples_so_far\n",
        "        if with_IoU_loss:\n",
        "            epoch_summary[\"training_IoU_coefficient\"] = training_IoU / examples_so_far\n",
        "\n",
        "\n",
        "        val_summary = evaluate_model(model, val_dataloader, epoch, epochs, criterion, with_dice_loss=with_dice_loss, with_IoU_loss=with_IoU_loss)\n",
        "        epoch_summary = {**epoch_summary, **val_summary}\n",
        "        returned_data.append(epoch_summary)\n",
        "\n",
        "    return returned_data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:53.225317Z",
          "iopub.execute_input": "2024-07-04T07:33:53.225664Z",
          "iopub.status.idle": "2024-07-04T07:33:53.243318Z",
          "shell.execute_reply.started": "2024-07-04T07:33:53.225632Z",
          "shell.execute_reply": "2024-07-04T07:33:53.242451Z"
        },
        "trusted": true,
        "id": "AsparATbsYU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inference and Visualization"
      ],
      "metadata": {
        "id": "u1h5SNJbsYU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_inference(batch, predictions):\n",
        "\n",
        "    batch_size = batch[0].shape[0]\n",
        "    fig, axes = plt.subplots(batch_size, 3, figsize=(6, 2.*batch_size), squeeze=True, sharey=True, sharex=True)\n",
        "    fig.subplots_adjust(hspace=0.05, wspace=0)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        img, mask = batch[0][i], batch[1][i]\n",
        "\n",
        "        axes[i, 0].imshow(decode_image(img.permute(1,2, 0)))\n",
        "        axes[i,0].set_xticks([])\n",
        "        axes[i,0].set_yticks([])\n",
        "        if i == 0:\n",
        "            axes[i, 0].set_title(\"Input Image\")\n",
        "\n",
        "        axes[i, 1].imshow(mask, cmap='magma')\n",
        "        axes[i,1].set_xticks([])\n",
        "        axes[i,1].set_yticks([])\n",
        "        if i == 0:\n",
        "            axes[i, 1].set_title(\"True Mask\")\n",
        "\n",
        "        predicted = predictions[i]\n",
        "        predicted = predicted.permute(1, 2, 0)\n",
        "        predicted = torch.argmax(predicted, dim=2)\n",
        "\n",
        "        axes[i, 2].imshow(predicted.cpu(), cmap='magma')\n",
        "        axes[i, 2].set_xticks([])\n",
        "        axes[i, 2].set_yticks([])\n",
        "        if i == 0:\n",
        "            axes[i, 2].set_title(\"Predicted Mask\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:53.244546Z",
          "iopub.execute_input": "2024-07-04T07:33:53.244837Z",
          "iopub.status.idle": "2024-07-04T07:33:53.261413Z",
          "shell.execute_reply.started": "2024-07-04T07:33:53.244813Z",
          "shell.execute_reply": "2024-07-04T07:33:53.260606Z"
        },
        "trusted": true,
        "id": "k3yqy-HosYU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the losses/coefficients"
      ],
      "metadata": {
        "id": "zEqb8T8isYU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses_coeffs(training_val_summary):\n",
        "\n",
        "    if cfg.SNS_STYLE is not None:\n",
        "        sns.set(style=cfg.SNS_STYLE)\n",
        "\n",
        "    training = defaultdict(list)\n",
        "    validation = defaultdict(list)\n",
        "\n",
        "    m = 0\n",
        "\n",
        "    for epoch_summary in training_val_summary:\n",
        "        keys = list(epoch_summary.keys())\n",
        "\n",
        "        training_keys = [k for k in keys if k.startswith(\"training\")]\n",
        "        for k in training_keys:\n",
        "            training[k[len(\"training\")+1:]].append(epoch_summary[k])\n",
        "\n",
        "        val_keys = [k for k in keys if k.startswith(\"validation\")]\n",
        "        for k in val_keys:\n",
        "            validation[k[len(\"validation\")+1:]].append(epoch_summary[k])\n",
        "\n",
        "    fig, axes = plt.subplots(len(validation.keys()), 1, figsize=(10, 10), sharex=True)\n",
        "\n",
        "    for i,k in enumerate(validation.keys()):\n",
        "\n",
        "        if k in training:\n",
        "            axes[i].plot(training[k], marker='o', linestyle='--', label=\"training\", linewidth=3)\n",
        "\n",
        "\n",
        "        if validation[k][-1] > validation[k][0]:\n",
        "            best_idx = np.argmax(validation[k])\n",
        "        else:\n",
        "            best_idx = np.argmin(validation[k])\n",
        "\n",
        "        best = validation[k][best_idx]\n",
        "\n",
        "        axes[i].set_title(f\"{k} (validation best : {round(best, 4)})\", fontsize=14, fontweight=\"bold\")\n",
        "        axes[i].plot(validation[k], label=\"validation\", marker='o', linestyle='--', linewidth=3)\n",
        "\n",
        "        axes[i].plot([0, best_idx], [best, best], linewidth=2, linestyle=\"--\", color='black', alpha=0.5)\n",
        "        m = max(m, len(validation[k]))\n",
        "        axes[i].legend()\n",
        "\n",
        "    axes[-1].set_xticks(list(range(0, m)))\n",
        "    axes[-1].set_xticklabels(list(range(1, m + 1)))\n",
        "    sns.set(style=\"white\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:53.262395Z",
          "iopub.execute_input": "2024-07-04T07:33:53.262653Z",
          "iopub.status.idle": "2024-07-04T07:33:53.275781Z",
          "shell.execute_reply.started": "2024-07-04T07:33:53.262631Z",
          "shell.execute_reply": "2024-07-04T07:33:53.27499Z"
        },
        "trusted": true,
        "id": "9pB7IFu2sYU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Definition: Unet"
      ],
      "metadata": {
        "id": "AHjlRKG1sYU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description\n",
        "\n",
        "Unet is one of the most popular architectures for semantic segmentation, which won multiple semantic segmentation competitions for medical images [1]. This architecture was introduced in the paper **\\\"U-Net: Convolutional Networks for Biomedical Image Segmentation\\\"**. The most likely reason why Unet is popular is that it works well and the architecture itself is easy to code and intuitive. It is a fully convolutional neural network (the main computational operation is 2D convolution, with no Dense layers) and the original architecture is represented in figure a). The architecture is nicely symmetric and intuitive. We start with an input image and apply convolutions twice. we then save this output, and downscale (using max-pooling layer), which results in an image of size $(\\frac{h}{2}, \\frac{w}{2})$. We continue to follow this pattern until we reach a bottleneck layer (bottom-middle layer). After applying 2 convolutions, we apply deconvolution (which will upscale the image, from $(h,w) \\rightarrow (2h, 2w)$). There a crucial step is done: the outputs from applying the convolutions (left-side) are concatenated with the deconvolutions (the upsampling). This is done in order to localize high resolution, and convolutions can learn to assemble a more precise output based on this information [1]. An important property of this architecture is that upsampling layers has also many channels, which should allow propagating information context properly. The architecture is visualized as shown below.\n",
        "\n",
        "![u-net_arch.png](attachment:570edd79-b64f-49a5-863b-528cf4487a40.png)\n",
        "\n",
        "Some other notes :\n",
        "- Unet paper also introduces a data augmentation for semantic segmentation. Data augmentation is a very popular technique to make the network generalize better, by perturbing or modifying the dataset. It also allows training networks with smaller datasets (as we constantly perturb the data). They achieve that by introducing **elastic deformations**.\n",
        "\n",
        "[1] https://arxiv.org/abs/1505.04597"
      ],
      "metadata": {
        "id": "sqELuAebsYU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network Architecture"
      ],
      "metadata": {
        "id": "psVEjC0GsYU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"apply twice convolution followed by batch normalization and relu. Preserves the width and height of input\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "\n",
        "        self.cn1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.activ1 = nn.ReLU(inplace=True)\n",
        "        self.cn2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.activ2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cn1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activ1(x)\n",
        "        x = self.cn2(x)\n",
        "        x = self.bn2(x)\n",
        "        return self.activ2(x)\n",
        "\n",
        "class DownScale(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then ConvBlock, transforming an input with (h, w, in_channels) to (h/2, w/2, out_channels)\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.block = ConvBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)\n",
        "        x = self.block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpScale(nn.Module):\n",
        "    \"\"\"apply upscaling and then convolution block transforming an input with (h,w,in_channels) to (2h, 2w, out_channels).\n",
        "       Forward function also simplifies Unet propagation by taking two inputs : first one from constantly propagating (from upscaling)\n",
        "       and the second one, which is the output from applying Downscale (first input is upscaled, then concatenated with second)\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "            self.conv = ConvBlock(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = ConvBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # input is (batch, channel, height, width)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, start=32, bilinear=False):\n",
        "        super(Unet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = ConvBlock(n_channels, start)\n",
        "        self.down1 = DownScale(start, 2*start)\n",
        "        self.down2 = DownScale(2*start, 4*start)\n",
        "        self.down3 = DownScale(4*start, 8*start)\n",
        "\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = DownScale(8*start, 16*start // factor)\n",
        "\n",
        "        self.up1 = UpScale(16*start, 8*start // factor, bilinear)\n",
        "        self.up2 = UpScale(8*start, 4*start // factor, bilinear)\n",
        "        self.up3 = UpScale(4*start, 2*start // factor, bilinear)\n",
        "        self.up4 = UpScale(2*start, start, bilinear)\n",
        "        self.outc = nn.Conv2d(start, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "number_of_classes = len(set(name_to_category.values()))\n",
        "summary(Unet(3, number_of_classes), input_data=eval_batch_data[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:53.27717Z",
          "iopub.execute_input": "2024-07-04T07:33:53.277838Z",
          "iopub.status.idle": "2024-07-04T07:33:53.928836Z",
          "shell.execute_reply.started": "2024-07-04T07:33:53.27781Z",
          "shell.execute_reply": "2024-07-04T07:33:53.92781Z"
        },
        "trusted": true,
        "id": "1wbWSeJVsYU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Training and Visualization\n",
        "\n",
        "We will now train the model on our dataset and evaluate the results.\n",
        "\n",
        "### Model Training"
      ],
      "metadata": {
        "id": "x2tUMPuwsYVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model = Unet(3, number_of_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "Unet_training_val_summary = train_model(model, device, train_dataloader, val_dataloader,\n",
        "                                        lr=3e-4, epochs=cfg.SINGLE_NETWORK_TRAINING_EPOCHS, update_pb_every_batch=10)\n",
        "\n",
        "clear_output(True)\n",
        "plot_losses_coeffs(Unet_training_val_summary)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:33:53.929887Z",
          "iopub.execute_input": "2024-07-04T07:33:53.930167Z",
          "iopub.status.idle": "2024-07-04T07:37:27.985963Z",
          "shell.execute_reply.started": "2024-07-04T07:33:53.930144Z",
          "shell.execute_reply": "2024-07-04T07:37:27.985036Z"
        },
        "trusted": true,
        "id": "cWHSHh4qsYVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inference (Show results)"
      ],
      "metadata": {
        "id": "-Fg8LhQpsYVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(val_dataloader))\n",
        "predictions = model(batch[0].to(device))\n",
        "show_inference(batch, predictions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:37:27.987161Z",
          "iopub.execute_input": "2024-07-04T07:37:27.987489Z",
          "iopub.status.idle": "2024-07-04T07:37:29.789573Z",
          "shell.execute_reply.started": "2024-07-04T07:37:27.987462Z",
          "shell.execute_reply": "2024-07-04T07:37:29.788616Z"
        },
        "trusted": true,
        "id": "JvfpQsQesYVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "## 6. Comparison of how loss affects the prediction (CE and DICE)\n",
        "\n",
        "In this section, we show simple visualization on prediction of these networks looks like when we prioritze differently CrossEntropy and DICE loss (a sweep of $\\beta \\in [0,1]$ for loss $\\beta \\cdot CrossEntropy(Y_{true}, Y_{pred}) + (1 - \\beta) \\cdot DICE(Y_{true}, Y_{pred})$)."
      ],
      "metadata": {
        "id": "2Lzv5A53sYVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training / Eval for CE vs DICE comparison"
      ],
      "metadata": {
        "id": "PN8bin2OsYVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_CE_DICE(model, device, train_dataloader,\n",
        "                val_dataloader, Beta=0., epochs=10, lr=1e-4, update_pb_every_batch = 1):\n",
        "    global global_step\n",
        "\n",
        "    # setup the optimizer, loss, learning rate scheduler\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
        "\n",
        "    returned_data = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        training_loss = 0\n",
        "        training_dice = 0\n",
        "        training_CE = 0\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        examples_so_far = 0\n",
        "        for i,batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            images, true_masks = batch[0], batch[1]\n",
        "\n",
        "            images = images.to(device)\n",
        "            true_masks = true_masks.to(device).long()\n",
        "\n",
        "            masks_pred = model(images)\n",
        "\n",
        "            CE   = cross_entropy(masks_pred, true_masks)\n",
        "            dice = dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
        "                              F.one_hot(true_masks.long(), number_of_classes).permute(0, 3, 1, 2).float())\n",
        "            loss = Beta * CE + (1. - Beta) * dice\n",
        "            training_dice += images.shape[0] * (1.-dice.item())\n",
        "            training_CE += images.shape[0] * CE.item()\n",
        "\n",
        "            # Backward and optimize\n",
        "            #loss.backward()\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "            global_step += 1\n",
        "            training_loss += images.shape[0] * loss.item()\n",
        "            examples_so_far += images.shape[0]\n",
        "\n",
        "        epoch_summary = {}\n",
        "        epoch_summary[\"training_loss\"] = training_loss / examples_so_far\n",
        "        epoch_summary[\"training_CE\"] = training_CE / examples_so_far\n",
        "        epoch_summary[\"training_DICE_coefficient\"] = training_dice / examples_so_far\n",
        "\n",
        "        val_summary = evaluate_model_CE_DICE(model, val_dataloader, epoch, epochs)\n",
        "        epoch_summary = {**epoch_summary, **val_summary}\n",
        "        returned_data.append(epoch_summary)\n",
        "\n",
        "    return returned_data\n",
        "\n",
        "\n",
        "def evaluate_model_CE_DICE(model, val_dataloader, epoch, epochs):\n",
        "    global epoch_to_fig\n",
        "    val_CE = 0\n",
        "    val_dice = 0\n",
        "    val_IoU = 0\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.eval()\n",
        "    examples_so_far = 0\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        images, true_masks = batch[0], batch[1]\n",
        "\n",
        "        images = images.to(device)\n",
        "        true_masks = true_masks.to(device).long()\n",
        "\n",
        "        # predictions\n",
        "        masks_pred = model(images)\n",
        "\n",
        "        loss = cross_entropy(masks_pred, true_masks)\n",
        "        val_CE += loss.item() * images.shape[0]\n",
        "        examples_so_far += images.shape[0]\n",
        "\n",
        "        dice = dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
        "                             F.one_hot(true_masks.long(), number_of_classes).permute(0, 3, 1, 2).float())\n",
        "        val_dice += images.shape[0] * (1.-dice.item())\n",
        "\n",
        "        IoU = IoU_loss(F.softmax(masks_pred, dim=1).float(),\n",
        "                           F.one_hot(true_masks.long(), number_of_classes).permute(0, 3, 1, 2).float())\n",
        "\n",
        "        val_IoU += images.shape[0] * (1.-IoU.item())\n",
        "\n",
        "    eval_summary = {}\n",
        "    eval_summary[\"validation_CE\"] = val_CE / examples_so_far\n",
        "    eval_summary[\"validation_DICE_coefficient\"] = val_dice / examples_so_far\n",
        "    eval_summary[\"validation_IoU_coefficient\"] = val_IoU / examples_so_far\n",
        "\n",
        "    return eval_summary"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:37:29.790995Z",
          "iopub.execute_input": "2024-07-04T07:37:29.79133Z",
          "iopub.status.idle": "2024-07-04T07:37:29.810365Z",
          "shell.execute_reply.started": "2024-07-04T07:37:29.791304Z",
          "shell.execute_reply": "2024-07-04T07:37:29.80943Z"
        },
        "trusted": true,
        "id": "dX1b4yo6sYVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CE vs DICE comparison plotting/training utils"
      ],
      "metadata": {
        "id": "5x9CcfdUsYVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_summaries_preds(summaries, preds):\n",
        "    true_val = eval_batch_data[1].permute(1, 2, 0)\n",
        "    images = [true_val] + [*preds]\n",
        "\n",
        "    batch_size = images[0].shape[-1]\n",
        "\n",
        "    fig, axes = plt.subplots(batch_size, len(images), figsize=(18, 2.*batch_size), squeeze=True, sharey=True, sharex=True)\n",
        "    fig.subplots_adjust(hspace=0.0, wspace=0)\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        for j in range(batch_size):\n",
        "\n",
        "            axes[j, i].imshow(images[i][:,:,j].cpu())\n",
        "            axes[j,i].set_xticks([])\n",
        "            axes[j,i].set_yticks([])\n",
        "            if i == 0 and j == 0:\n",
        "                summary = summaries[i]\n",
        "                axes[0,0].set_title(f\"True Mask\")\n",
        "\n",
        "            elif i == 1 and j == 0:\n",
        "                axes[j,i].set_title(f\"Sum of DICE+CE\\nCE={round(summary['VAL_LOSS'],2)},DICE={round(summary['VAL_DICE'],2)}\")\n",
        "\n",
        "            elif j == 0:\n",
        "                summary = summaries[i-1]\n",
        "                axes[j,i].set_title(f\"Beta={round(summary['BETA'], 2)}\\nCE={round(summary['VAL_CE'],2)},DICE={round(summary['VAL_DICE'],2)}\")\n",
        "\n",
        "\n",
        "def get_summaries_preds_for_comparison(delta_Beta, model_ref, model_params, model_mixed, summary_mixed, epochs=25, lr=3e-4):\n",
        "\n",
        "    summaries = []\n",
        "    predictions = []\n",
        "    device = \"cuda\"\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for x in summary_mixed:\n",
        "        for k,v in x.items():\n",
        "            d[k].append(v)\n",
        "\n",
        "    LOSS, DICE, V_LOSS, V_DICE, V_IOU = d[\"training_loss\"][-1], d[\"training_DICE_coefficient\"][-1], d[\"validation_loss\"][-1], d[\"validation_DICE_coefficient\"][-1], d[\"validation_IoU_coefficient\"][-1]\n",
        "    summaries.append({\"LOSS\" : LOSS, \"DICE\" : DICE, \"VAL_LOSS\" : V_LOSS, \"VAL_DICE\" : V_DICE, \"VAL_IOU\" : V_IOU})\n",
        "    predictions.append(torch.argmax(model_mixed(eval_batch_data[0].to(device)), dim=1).permute(1, 2, 0))\n",
        "\n",
        "    for Beta in np.arange(0, 1. + 1e-9, delta_Beta):\n",
        "        start = time()\n",
        "        model = model_ref(**model_params)\n",
        "        model = model.to(device)\n",
        "        summary = train_model_CE_DICE(model, device, train_dataloader, val_dataloader, Beta=Beta,\n",
        "                                                lr=lr, epochs=epochs, update_pb_every_batch=10)\n",
        "\n",
        "        d = defaultdict(list)\n",
        "        for x in summary:\n",
        "            for k,v in x.items():\n",
        "                d[k].append(v)\n",
        "\n",
        "        CE, DICE, V_CE, V_DICE, V_IOU = d[\"training_CE\"][-1], d[\"training_DICE_coefficient\"][-1], d[\"validation_CE\"][-1], d[\"validation_DICE_coefficient\"][-1], d[\"validation_IoU_coefficient\"][-1]\n",
        "\n",
        "        summaries.append({\"BETA\" : Beta,  \"CE\" : CE, \"DICE\" : DICE, \"VAL_CE\" : V_CE, \"VAL_DICE\" : V_DICE, \"VAL_IOU\" : V_IOU})\n",
        "        predictions.append(torch.argmax(model(eval_batch_data[0].to(device)), dim=1).permute(1, 2, 0))\n",
        "        print(f\"[{int(time() - start)}s] Model with Beta={Beta} finished with training CE : {CE}, training DICE : {DICE}, validation CE : {V_CE}, Validation DICE : {V_DICE}, Validation IOU : {V_IOU}\")\n",
        "\n",
        "\n",
        "    return summaries, predictions\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:37:29.811545Z",
          "iopub.execute_input": "2024-07-04T07:37:29.81235Z",
          "iopub.status.idle": "2024-07-04T07:37:29.831329Z",
          "shell.execute_reply.started": "2024-07-04T07:37:29.812315Z",
          "shell.execute_reply": "2024-07-04T07:37:29.830334Z"
        },
        "trusted": true,
        "id": "VFksqGV6sYVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LONG COMPUTATION !\n",
        "\n",
        "summaries, preds = get_summaries_preds_for_comparison(cfg.DELTA_BETA, Unet, {\"n_channels\" : 3, \"n_classes\" : number_of_classes},\n",
        "                                                      model_mixed=model, summary_mixed=Unet_training_val_summary, epochs=cfg.CE_VS_DICE_EVAL_EPOCHS)\n",
        "\n",
        "plot_summaries_preds(summaries, preds)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T07:37:29.832304Z",
          "iopub.execute_input": "2024-07-04T07:37:29.832592Z",
          "iopub.status.idle": "2024-07-04T07:57:39.585107Z",
          "shell.execute_reply.started": "2024-07-04T07:37:29.832568Z",
          "shell.execute_reply": "2024-07-04T07:57:39.584207Z"
        },
        "trusted": true,
        "id": "CJsDfDKisYVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the DICE loss gives superior results than a simple cross-entropy loss"
      ],
      "metadata": {
        "id": "g08nt-VtsYVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise\n",
        "1. You can compare the result of DICE vs IOU loss."
      ],
      "metadata": {
        "id": "fXusfBNtsYVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Useful Resources for Semantic Segmentation\n",
        "1. https://github.com/open-mmlab/mmsegmentation\n",
        "2. https://github.com/qubvel/segmentation_models.pytorch"
      ],
      "metadata": {
        "id": "m3euNiwSsYVB"
      }
    }
  ]
}